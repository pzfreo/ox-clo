

```python
from pyspark.sql import SQLContext
sqlc = SQLContext(sc)
df = sqlc.read.csv('file:///home/oxclo/datafiles/incidents/sfpd.csv', header='true', inferSchema='true')
df = df.select(df.Y, df.X)

from numpy import array
geoarray = df.rdd.map(lambda (y,x): array([y,x]))

from pyspark.mllib.clustering import KMeans, KMeansModel
numclusters = 5
clusters = KMeans.train(geoarray, numclusters, maxIterations=10, runs=10, initializationMode="random")

for arr in clusters.centers:
    list = arr.tolist()
    print str(list[0]) + "," + str(list[1])
    
# Example output:
# 37.7404230878,-122.475644308
# 37.7364153616,-122.40905349
# 90.0,-120.5
# 37.7763006544,-122.444078621
# 37.782502841,-122.411219809
    
# Used https://mapmakerapp.com instead of hamstermap.com to plot the geo coordinates



```

    90.0,-120.5
    37.7504601866,-122.473703105
    37.7827927436,-122.431124844
    37.7798298405,-122.408852037
    37.7338105572,-122.410500962

