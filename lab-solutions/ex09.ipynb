{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  date|    id|                name|            address1|            address2|                town|            district|            postcode|               extra|\n",
      "+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|201512|A81001|THE DENSHAM SURGE...|THE HEALTH CENTRE...|LAWSON STREET    ...|STOCKTON ON TEES ...|CLEVELAND        ...|TS18 1HU         ...|                 ...|\n",
      "|201512|A81002|QUEENS PARK MEDIC...|QUEENS PARK MEDIC...|FARRER STREET    ...|STOCKTON ON TEES ...|CLEVELAND        ...|TS18 2AW         ...|                 ...|\n",
      "|201512|A81003|VICTORIA MEDICAL ...|THE HEALTH CENTRE...|VICTORIA ROAD    ...|HARTLEPOOL       ...|CLEVELAND        ...|TS26 8DB         ...|                 ...|\n",
      "|201512|A81004|WOODLANDS ROAD SU...|6 WOODLANDS ROAD ...|                 ...|MIDDLESBROUGH    ...|CLEVELAND        ...|TS1 3BE          ...|                 ...|\n",
      "|201512|A81005|SPRINGWOOD SURGER...|SPRINGWOOD SURGER...|RECTORY LANE     ...|GUISBOROUGH      ...|                 ...|TS14 7DJ         ...|                 ...|\n",
      "+------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/practices/ukpractices2015.csv', header='true', inferSchema='true')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SW11 15\n",
      "OX1 7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/practices/ukpractices2015.csv', header='true', inferSchema='true')\n",
    "# df.show(5)\n",
    "\n",
    "split_col = split(df['postcode'], ' ')\n",
    "df = df.withColumn('post', split_col.getItem(0))\n",
    "\n",
    "# Use Functional approach\n",
    "mapped = df.rdd.map(lambda x: (x.post, 1)).filter(lambda x: 'OX1' in x or 'SW11' in x)\n",
    "# reduce style\n",
    "count = mapped.reduceByKey(lambda a,b: a+b)\n",
    "sortedCount = count.sortBy(lambda (a,b): b, ascending=False)\n",
    "for k,v in sortedCount.collect(): print k,v\n",
    "## countByKey style - note it returns a python object  \n",
    "## count = mapped.countByKey().items()\n",
    "## for k,v in count: print k,v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|post|sum|\n",
      "+----+---+\n",
      "|SW11| 15|\n",
      "| OX1|  7|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/practices/ukpractices2015.csv', header='true', inferSchema='true')\n",
    "# df.show(5)\n",
    "\n",
    "split_col = split(df['postcode'], ' ')\n",
    "df = df.withColumn('post', split_col.getItem(0))\n",
    "\n",
    "# Use SQL approach\n",
    "df.registerTempTable('practices')\n",
    "sqlc.sql(\"Select post, count(post) as sum from practices where post in ('OX1', 'SW11') group by post order by post desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  pc|sum(1)|\n",
      "+----+------+\n",
      "|SW11|    15|\n",
      "| OX1|     7|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/practices/ukpractices2015.csv', header='true', inferSchema='true')\n",
    "# df.show(5)\n",
    "\n",
    "sqlc.registerFunction('f', lambda p: p.split()[0])\n",
    "                                              \n",
    "# split_col = split(df['postcode'], ' ')\n",
    "# df = df.withColumn('post', split_col.getItem(0))\n",
    "\n",
    "# Use SQL approach\n",
    "df.registerTempTable('practices')\n",
    "sqlc.sql(\"Select f(postcode) as pc, sum(1) from practices where f(postcode) in ('OX1', 'SW11') group by pc order by pc desc\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
