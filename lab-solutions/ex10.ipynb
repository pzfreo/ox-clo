{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cd ~/datafiles\n",
    "# mkdir wind2014\n",
    "# cd wind2014\n",
    "# wget http://freo.me/1LpKbGV -O wd2014.zip\n",
    "# unzip wd2014.zip\n",
    "# rm wd2014.zip\n",
    "\n",
    "\n",
    "# dt = parse(\"datestring\")  # returns dateime.datetime\n",
    "# def date_and_hour(s):\n",
    "#     dt = parse(s.replace('?',''))\n",
    "#     hour = dt.hour\n",
    "#     return (dt.strftime(\"%Y-%m-%d\"), hour)\n",
    "\n",
    "\n",
    "# Incidents Schema\n",
    "# |IncidntNum|Category|Descript|DayOfWeek|Date|Time|PdDistrict|Resolution|Address|X|Y|Location|PdId|\n",
    "# Wind Schema\n",
    "# |Station_ID|Station_Name|Location_Label|Interval_Minutes|Interval_End_Time|Wind_Velocity_Mtr_Sec|\n",
    "#     Wind_Direction_Variance_Deg|Wind_Direction_Deg|Ambient_Temperature_Deg_C|Global_Horizontal_Irradiance|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+----------------+-----------------+---------------------+---------------------------+------------------+-------------------------+----------------------------+\n",
      "|Station_ID|        Station_Name|Location_Label|Interval_Minutes|Interval_End_Time|Wind_Velocity_Mtr_Sec|Wind_Direction_Variance_Deg|Wind_Direction_Deg|Ambient_Temperature_Deg_C|Global_Horizontal_Irradiance|\n",
      "+----------+--------------------+--------------+----------------+-----------------+---------------------+---------------------------+------------------+-------------------------+----------------------------+\n",
      "|      SF15|Warnerville Switc...|   Warnerville|               5| 2014-01-4? 00:05|                0.671|                       9.09|              91.3|                    3.466|                       0.045|\n",
      "+----------+--------------------+--------------+----------------+-----------------+---------------------+---------------------------+------------------+-------------------------+----------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/wind2014/*.csv', header='true', inferSchema='true')\n",
    "# df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------------------+------------------+\n",
      "|station|     zdate|zhour|          avg(vel)|          avg(deg)|\n",
      "+-------+----------+-----+------------------+------------------+\n",
      "|   SF04|2014-01-01|    8|          2.236375|            12.085|\n",
      "|   SF04|2014-01-01|    9|1.5883000000000003|            13.097|\n",
      "|   SF04|2014-01-01|   10|2.1740833333333334|14.726041666666662|\n",
      "|   SF04|2014-01-01|   11|2.1182083333333335|16.376458333333332|\n",
      "|   SF04|2014-01-01|   12| 2.084979166666667|16.627708333333334|\n",
      "|   SF04|2014-01-01|   13|1.7878958333333337| 17.40083333333334|\n",
      "|   SF04|2014-01-01|   14|1.5741249999999998| 17.80083333333333|\n",
      "|   SF04|2014-01-01|   15|1.7833750000000002|16.636458333333334|\n",
      "|   SF04|2014-01-01|   16|1.6852708333333328|         15.145625|\n",
      "|   SF04|2014-01-01|   17| 1.765552631578947|14.621842105263156|\n",
      "+-------+----------+-----+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "df = sqlc.read.csv('file:///home/oxclo/datafiles/wind2014/*.csv', header='true', inferSchema='true')\n",
    "\n",
    "df = df.withColumn('Interval_End_Time', regexp_replace(df.Interval_End_Time, '(.*)\\\\? (.*)', '$1 $2'))\n",
    "df = df.withColumn('zdate', to_date(df.Interval_End_Time)).withColumn('zhour', hour(df.Interval_End_Time))\n",
    "\n",
    "df = df.filter(\"Wind_Velocity_Mtr_Sec > 0.0 and Ambient_Temperature_Deg_C > 0.0\")\\\n",
    ".dropna(subset=['Wind_Velocity_Mtr_Sec', \"Ambient_Temperature_Deg_C\", \"Interval_End_Time\", \"Interval_Minutes\"])\\\n",
    ".withColumnRenamed('Station_ID', 'station')\\\n",
    ".withColumnRenamed('Wind_Velocity_Mtr_Sec', 'vel')\\\n",
    ".withColumnRenamed('Ambient_Temperature_Deg_C', 'deg')\n",
    "\n",
    "df = df.select(df.station, df.zdate, df.zhour, df.vel, df.deg).orderBy(df.station, df.zdate, df.zhour)\n",
    "df = df.groupBy(df.station, df.zdate, df.zhour).avg('vel', 'deg').orderBy(df.station, df.zdate, df.zhour)\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+---------+\n",
      "|station|     zdate|zhour|incidents|\n",
      "+-------+----------+-----+---------+\n",
      "|   SF37|2014-01-01|   00|       38|\n",
      "|   SF37|2014-06-29|   17|       34|\n",
      "|   SF37|2014-09-19|   17|       32|\n",
      "|   SF37|2014-02-24|   17|       30|\n",
      "|   SF37|2014-02-13|   15|       30|\n",
      "|   SF37|2014-08-29|   23|       28|\n",
      "|   SF37|2014-03-06|   16|       27|\n",
      "|   SF37|2014-02-01|   00|       27|\n",
      "|   SF37|2014-08-08|   18|       26|\n",
      "|   SF37|2014-06-29|   18|       26|\n",
      "|   SF37|2014-08-26|   19|       26|\n",
      "|   SF37|2014-07-30|   17|       25|\n",
      "|   SF37|2014-11-25|   18|       25|\n",
      "|   SF37|2014-03-14|   17|       24|\n",
      "|   SF37|2014-10-21|   18|       24|\n",
      "+-------+----------+-----+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import KDTree\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)\n",
    "\n",
    "sfpds = sqlc.read.csv('file:///home/oxclo/datafiles/incidents/sfpd.csv', header='true', inferSchema='true')\n",
    "sfpds = sfpds.withColumn('zdate', to_date(from_unixtime(unix_timestamp(sfpds.Date, 'MM/dd/yyy'))))\\\n",
    ".withColumn('zhour', regexp_extract(sfpds.Time, '(\\d+):(\\d+)', 1))\n",
    "\n",
    "sfpds = sfpds.filter(year(sfpds.zdate) == 2014)\n",
    "\n",
    "# NOTE: stations and their locations info from http://freo.me/oxclo-locate\n",
    "def to_station(y,x):\n",
    "    station_locations = [\\\n",
    "    [37.7816834,-122.3887657],[37.7469112,-122.4821759],[37.7411022,-120.8041510],[37.4834543,-122.3187302],\\\n",
    "    [37.7576436,-122.3916382],[37.7970013,-122.4140409],[37.7484960,-122.4567461],[37.7288155,-122.4210133],\\\n",
    "    [37.5839487,-121.9499339],[37.7157156,-122.4145311],[37.7329613,-122.5051491],[37.7575891,-122.3923824],\\\n",
    "    [37.7521169,-122.4497687]]\n",
    "    stations = [\"SF18\", \"SF04\", \"SF15\", \"SF17\", \"SF36\", \"SF37\",\"SF07\", \"SF11\", \"SF12\", \"SF14\", \"SF16\", \"SF19\", \"SF34\"]\n",
    "    tree = KDTree(station_locations)\n",
    "    return stations[tree.query([y, x])[1]]\n",
    "\n",
    "kdtree_udf = udf(to_station)\n",
    "sfpds = sfpds.withColumn('station', kdtree_udf(sfpds.Y, sfpds.X))\n",
    "\n",
    "sfpds = sfpds.select(sfpds.station, sfpds.zdate, sfpds.zhour)\\\n",
    ".groupBy(sfpds.station, sfpds.zdate, sfpds.zhour)\\\n",
    ".agg(count('*').alias('incidents'))\\\n",
    ".orderBy('incidents', ascending=False)\n",
    "sfpds.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------------------+------------------+---+\n",
      "|station|     zdate|zhour|                 t|                 w|  i|\n",
      "+-------+----------+-----+------------------+------------------+---+\n",
      "|   SF37|2014-01-01|    0|11.945208333333333|0.8127083333333336| 38|\n",
      "|   SF37|2014-03-06|   16|15.738541666666668| 4.125854166666667| 27|\n",
      "|   SF37|2014-02-01|    0|11.591249999999997| 2.073729166666667| 27|\n",
      "|   SF37|2014-06-04|   12|18.386666666666667| 4.349277777777776| 22|\n",
      "|   SF37|2014-06-06|   20|13.848541666666664|3.5619374999999986| 22|\n",
      "|   SF37|2014-02-01|   23|10.405444444444441| 1.810311111111111| 22|\n",
      "|   SF37|2014-07-02|   10| 17.80416666666667|         3.2446875| 22|\n",
      "|   SF37|2014-04-05|   20|         12.803125| 2.430583333333333| 22|\n",
      "|   SF37|2014-01-06|   20|12.648245614035089|1.4235614035087716| 21|\n",
      "|   SF37|2014-11-07|   17|17.531166666666667|1.2534666666666667| 21|\n",
      "|   SF37|2014-04-01|   10|17.093750000000007|1.9050208333333327| 21|\n",
      "|   SF37|2014-11-01|    1| 13.90016666666666|1.4941166666666674| 21|\n",
      "|   SF37|2014-05-02|   17|17.363541666666666| 3.071562500000001| 21|\n",
      "|   SF37|2014-01-01|    1| 11.30238095238095|0.8441666666666668| 21|\n",
      "|   SF37|2014-03-02|   15|16.045000000000005|           2.34185| 20|\n",
      "+-------+----------+-----+------------------+------------------+---+\n",
      "only showing top 15 rows\n",
      "\n",
      "[[1.         0.15930172 0.14646246]\n",
      " [0.15930172 1.         0.04092338]\n",
      " [0.14646246 0.04092338 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "cond = [df.station == sfpds.station, df.zdate == sfpds.zdate, df.zhour == sfpds.zhour]\n",
    "joined = df.join(sfpds, cond)\\\n",
    ".select(df.station, df.zdate, df.zhour, 'avg(deg)', 'avg(vel)', sfpds.incidents)\\\n",
    ".withColumnRenamed('avg(deg)', 't')\\\n",
    ".withColumnRenamed('avg(vel)', 'w')\\\n",
    ".withColumnRenamed('incidents', 'i')\\\n",
    ".orderBy(sfpds.incidents, ascending=False)\n",
    "\n",
    "joined.show(15)\n",
    "\n",
    "# remap the data into a vector of [t,w,i]\n",
    "vecs = joined.rdd.map(lambda x: Vectors.dense(x.t, x.w, x.i))\n",
    "print(Statistics.corr(vecs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
